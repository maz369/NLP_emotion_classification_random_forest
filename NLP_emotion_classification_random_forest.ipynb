{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "### Training a model for text classification based on dominant emotion.\n",
    "<br>\n",
    "\n",
    "## Data:\n",
    "### 47288 tweets scrapped from twitter API with corresponding labels. Lebels are representing 5 classes of emotions including: Neutral üòê,  Happines üòÇ,  Fear üò±,  Hate üòí,  Anger üò†\n",
    "Numerical value corresponding to neutral, happines, fear, hate and anger are 0, 1, 2, 3 and 4, respectively.\n",
    "<br>\n",
    "\n",
    "## Description:\n",
    "### Multi-class classification will be preformed using random forest classifier based on n-grams (n=4) and grid search library from Scikit-learn will be used for hyperparamter tuning.  \n",
    "*Note the data for this project has been scarapped from twitter and pre-processing was performed to clean the data and make it ready for the analysis. I am plannig on releasing the code for scraping and preprocessing shortly.\n",
    "<br>\n",
    "\n",
    "## Result:\n",
    "### F1-score is used as a measure of classification accuracy. Weighted F1 for the 5 classes is 0.48.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import nltk\n",
    "import pkg_resources\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing\n",
    "- Spell correction\n",
    "- Lower case letters\n",
    "- Punctuation removal\n",
    "- Correct letter repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell correction is based on SymSpell Python library. This tool checks for possible spelling errors within a maximum \n",
    "edit distance of n (N-3 in this work) using Fuzzy logic.<br>\n",
    "Use NLTK library is used for preprocessing and preparing sentences for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup path and parameters for spell correction by symspell libray\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "\n",
    "# setup max edit distance\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "# path for dictionary\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "\n",
    "# term_index is the column of the term and count_index is the column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Mk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "# correct letter repetitions\n",
    "def de_repeat(text):\n",
    "    ulist = []\n",
    "    [ulist.append(x) for x in text if x not in ulist]\n",
    "    return ulist\n",
    "    \n",
    "# perform preprocessing    \n",
    "def pre_process(text):\n",
    "    text.lower()\n",
    "    text.split()   \n",
    "    text = ' '.join(de_repeat(text.split()))\n",
    "    text_spell_corr = sym_spell.word_segmentation(text)\n",
    "    return text_spell_corr.corrected_string\n",
    "    \n",
    "# toy example to test functionality of the pre-processing step    \n",
    "# pre_process(\"whau you do do is coool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select tweets and convert them to list\n",
    "sentences = data['sentence'].values.tolist()\n",
    "emotions = data['emotion'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(token, n): \n",
    "    output = []\n",
    "    for i in range(n-1, len(token)): \n",
    "        ngram = ' '.join(token[i-n+1:i+1])\n",
    "        output.append(ngram) \n",
    "    return output\n",
    "\n",
    "\n",
    "def create_feature(text, nrange=(1, 1)):\n",
    "    text_features = [] \n",
    "    text = str(text).lower() \n",
    "\n",
    "    # 1. treat alphanumeric characters as word tokens\n",
    "    # Since tweets contain #, we keep it as a feature\n",
    "    # Then, extract all ngram lengths\n",
    "    text_alphanum = re.sub('[^a-z0-9#]', ' ', text)\n",
    "    for n in range(nrange[0], nrange[1]+1): \n",
    "        text_features += ngram(text_alphanum.split(), n)\n",
    "    \n",
    "    # 2. treat punctuations as word token\n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    text_features += ngram(text_punc.split(), 1)\n",
    "    \n",
    "    # 3. Return a dictinaory whose keys are the list of elements \n",
    "    # and their values are the number of times appearede in the list.\n",
    "    return Counter(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate n-gram for the data\n",
    "sentences_ngram = []\n",
    "for i in range(len(sentences)):\n",
    "    text = sentences[i]\n",
    "    sentences_ngram.append(create_feature(text, nrange=(1, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for training\n",
    "- Peform train/test split 80/20\n",
    "- Vectorize sentences\n",
    "- Define accuracy measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train (80%) and test (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(sentences_ngram, emotions, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert feature list (dict) to feature value\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calu accuracy measure calculation\n",
    "def train_test(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    # weighted f1-score is used to find average f1 for each class by number \n",
    "    # of true instance per group to count for label imbalance\n",
    "    train_acc = f1_score(y_train, clf.predict(X_train), average='weighted')\n",
    "    test_acc = f1_score(y_test, clf.predict(X_test), average='weighted')   \n",
    "    print(\"Train acc: {}\".format(np.round(train_acc, 2)))\n",
    "    print(\"Test acc : {}\".format(np.round(test_acc, 2)))\n",
    "    return train_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & fine tune model\n",
    "- Train a vanilla random forest classifier\n",
    "- Perform grid search for hyperparameter tuning and avoiding overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.51\n",
      "Test acc : 0.48\n"
     ]
    }
   ],
   "source": [
    "# define and fit a random forest classifier with 450 trees\n",
    "forest_clf = RandomForestClassifier(max_depth=200, n_estimators=450,\n",
    "                                    max_leaf_nodes=200, n_jobs=-1, random_state=101)\n",
    "\n",
    "train_acc, test_acc = train_test(forest_clf, X_train_vec, X_test_vec, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a random forest with defult parameter setting results in overfitting. Possible solution is to limit depth of the tree or increase minimum number of samples per split. The following code will perform grid search to find the best setting of hyperparameters within the range of pre-defined parameters.Dominant hyperparameters for this experiment are found to be: \n",
    "- Depth of the tree\n",
    "- Number of trees \n",
    "- Max number of leaf per node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grid search for hyperparameter tuning.\n",
    "\n",
    "# define grid search parameters\n",
    "param_grid = {'max_depth': [100, 200, 300],\n",
    "              'n_estimators': [300, 500, 700],\n",
    "              'max_leaf_nodes': [100, 200],\n",
    "              'min_samples_split': [2, 4, 6]}\n",
    "\n",
    "# perform grid search by 3-fold cross-validation while using \n",
    "# 9 CPU workers (use -1 to utilize all available resourses)\n",
    "grid_search = GridSearchCV(forest_clf, param_grid, cv=3, verbose=1, n_jobs=9)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best setting of hyperparameters:')\n",
    "# best parameter found by the grid search \n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# best accuracy score obtained by the grid search (corresponding to the above best parameters)\n",
    "print('Best accuracy score is: {}'.format(np.round(grid_search.best_score_, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=200, max_leaf_nodes=200, n_estimators=450,\n",
       "                       n_jobs=-1, random_state=101)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train random forest with final setting of hyperparameters\n",
    "forest_clf.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define emoji dictionary\n",
    "emoji_dict = {0:\"üòê\", 1:\"üòÇ\", 2:\"üò±\", 3:\"üòí\", 4:\"üò†\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_emoji(text):\n",
    "    text_preprocess = pre_process(text)\n",
    "    sentence_ngram = create_feature(text_preprocess, nrange=(1, 4))\n",
    "    sentence_ngram_vec = vectorizer.transform(sentence_ngram)\n",
    "    pred = forest_clf.predict(sentence_ngram_vec)\n",
    "    return emoji_dict[pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üòÇ'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'He is a happy man'\n",
    "suggest_emoji(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üò±'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'you re missing the devil wears prada sad'\n",
    "suggest_emoji(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üò†'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"the dance video that haunts my dreams india is confused angry wtf more cheese than a bollywood d movie\"\n",
    "suggest_emoji(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
